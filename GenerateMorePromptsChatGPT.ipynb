{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from beeprint import pp\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Section</th>\n",
       "      <th>Criteria</th>\n",
       "      <th>Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auto_grading_trainingset\\input_files\\input.txt</td>\n",
       "      <td>Description</td>\n",
       "      <td>1. Does the name of the \"Project Name:\" attrib...</td>\n",
       "      <td>\\nYou are grading a project Description. This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auto_grading_trainingset\\input_files\\input.txt</td>\n",
       "      <td>Description</td>\n",
       "      <td>2. Does the name of the \"Clinic:\" attribute sp...</td>\n",
       "      <td>\\nYou are grading a project Description. This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto_grading_trainingset\\input_files\\input.txt</td>\n",
       "      <td>Description</td>\n",
       "      <td>3. Does the name of the \"Process:\" attribute s...</td>\n",
       "      <td>\\nYou are grading a project Description. This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>auto_grading_trainingset\\input_files\\input.txt</td>\n",
       "      <td>Description</td>\n",
       "      <td>4. Does the name of the \"TIP 2.0 Process Miles...</td>\n",
       "      <td>\\nYou are grading a project Description. This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>auto_grading_trainingset\\input_files\\input.txt</td>\n",
       "      <td>Description</td>\n",
       "      <td>5. Does the name of the \"TIP 2.0 Process Miles...</td>\n",
       "      <td>\\nYou are grading a project Description. This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>auto_grading_trainingset\\input_files\\input9.txt</td>\n",
       "      <td>Team</td>\n",
       "      <td>4. Does the value of the \"Process Manager:\" at...</td>\n",
       "      <td>\\nYou are grading a project Team. This is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>auto_grading_trainingset\\input_files\\input9.txt</td>\n",
       "      <td>Team</td>\n",
       "      <td>5. Does the value of the \"Stakeholders\" \"stake...</td>\n",
       "      <td>\\nYou are grading a project Team. This is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>auto_grading_trainingset\\input_files\\input9.txt</td>\n",
       "      <td>Team</td>\n",
       "      <td>6. Do the \"Stakeholders\" contain values for th...</td>\n",
       "      <td>\\nYou are grading a project Team. This is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>auto_grading_trainingset\\input_files\\input9.txt</td>\n",
       "      <td>Team</td>\n",
       "      <td>7. Does the value of the \"Project Team Members...</td>\n",
       "      <td>\\nYou are grading a project Team. This is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>auto_grading_trainingset\\input_files\\input9.txt</td>\n",
       "      <td>Team</td>\n",
       "      <td>8. Do the \"Project Team Members\" contain role ...</td>\n",
       "      <td>\\nYou are grading a project Team. This is the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename      Section  \\\n",
       "0     auto_grading_trainingset\\input_files\\input.txt  Description   \n",
       "1     auto_grading_trainingset\\input_files\\input.txt  Description   \n",
       "2     auto_grading_trainingset\\input_files\\input.txt  Description   \n",
       "3     auto_grading_trainingset\\input_files\\input.txt  Description   \n",
       "4     auto_grading_trainingset\\input_files\\input.txt  Description   \n",
       "..                                               ...          ...   \n",
       "331  auto_grading_trainingset\\input_files\\input9.txt         Team   \n",
       "332  auto_grading_trainingset\\input_files\\input9.txt         Team   \n",
       "333  auto_grading_trainingset\\input_files\\input9.txt         Team   \n",
       "334  auto_grading_trainingset\\input_files\\input9.txt         Team   \n",
       "335  auto_grading_trainingset\\input_files\\input9.txt         Team   \n",
       "\n",
       "                                              Criteria  \\\n",
       "0    1. Does the name of the \"Project Name:\" attrib...   \n",
       "1    2. Does the name of the \"Clinic:\" attribute sp...   \n",
       "2    3. Does the name of the \"Process:\" attribute s...   \n",
       "3    4. Does the name of the \"TIP 2.0 Process Miles...   \n",
       "4    5. Does the name of the \"TIP 2.0 Process Miles...   \n",
       "..                                                 ...   \n",
       "331  4. Does the value of the \"Process Manager:\" at...   \n",
       "332  5. Does the value of the \"Stakeholders\" \"stake...   \n",
       "333  6. Do the \"Stakeholders\" contain values for th...   \n",
       "334  7. Does the value of the \"Project Team Members...   \n",
       "335  8. Do the \"Project Team Members\" contain role ...   \n",
       "\n",
       "                                                Prompt  \n",
       "0    \\nYou are grading a project Description. This ...  \n",
       "1    \\nYou are grading a project Description. This ...  \n",
       "2    \\nYou are grading a project Description. This ...  \n",
       "3    \\nYou are grading a project Description. This ...  \n",
       "4    \\nYou are grading a project Description. This ...  \n",
       "..                                                 ...  \n",
       "331  \\nYou are grading a project Team. This is the ...  \n",
       "332  \\nYou are grading a project Team. This is the ...  \n",
       "333  \\nYou are grading a project Team. This is the ...  \n",
       "334  \\nYou are grading a project Team. This is the ...  \n",
       "335  \\nYou are grading a project Team. This is the ...  \n",
       "\n",
       "[336 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_prompts_df = pd.read_csv('all_prompts_df.csv')\n",
    "all_prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_prompts(prompt):\n",
    "    # Function to generate three new prompts based on the given prompt\n",
    "    new_prompts = []\n",
    "    \n",
    "    # Example logic to generate new prompts (this should be replaced with actual logic)\n",
    "    for i in range(1, 4):\n",
    "        new_prompt = f\"{prompt.strip()} (Variation {i})\"\n",
    "        new_prompts.append(new_prompt)\n",
    "    \n",
    "    return new_prompts\n",
    "\n",
    "# Create a new DataFrame to store the new prompts\n",
    "new_prompts_df = pd.DataFrame(columns=[\"Original Prompt\", \"New Prompt 1\", \"New Prompt 2\", \"New Prompt 3\"])\n",
    "\n",
    "# Iterate over each prompt in the original DataFrame\n",
    "for index, row in all_prompts_df.iterrows():\n",
    "    original_prompt = row['Prompt']\n",
    "    new_prompts = generate_new_prompts(original_prompt)\n",
    "    \n",
    "    # Add the new prompts to the DataFrame\n",
    "    new_prompts_df.loc[len(new_prompts_df)] = [original_prompt] + new_prompts\n",
    "\n",
    "# Display the new DataFrame\n",
    "new_prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from Helper.logging import langsmith\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "output_dir = \"chunk\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Load rubric from the YAML file\n",
    "def load_rubric(filename: str):\n",
    "    with open(filename, 'r') as file:\n",
    "        rubric = yaml.safe_load(file)\n",
    "    return rubric\n",
    "\n",
    "\n",
    "# Dynamically generate the prompt based on the loaded rubric\n",
    "def generate_prompt(rubric: dict, part_name: str):\n",
    "    if part_name not in rubric:\n",
    "        return f\"No rubric found for {part_name}.\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    You are an evaluator. Below is a rubric for evaluating various parts of a project report. Please evaluate each part based on the specific criteria provided for each section.\n",
    "    \"\"\"\n",
    "\n",
    "    for criterion in rubric[part_name]['criteria']:\n",
    "        prompt += f\"- {criterion['name']} (Score Range: {criterion['grade_range']})\\n\"\n",
    "\n",
    "    prompt += f\"\"\"\n",
    "    Instructions:\n",
    "    1. Assign a score based on the criteria and provided grade range.\n",
    "    2. Provide a brief explanation if a section does not meet the criteria.\n",
    "    3. Please return the result in JSON format, including the score for each section.\n",
    "        - Here is the required format:\n",
    "        {{\n",
    "            \"Project Description / Purpose\": {{\n",
    "                \"score\": \"\" if part_name != \"Project Description / Purpose\" else \" \",\n",
    "                \"explanation\": \"\" if part_name != \"Project Description / Purpose\" else \" \"\n",
    "            }},\n",
    "            \"Project Overview\": {{\n",
    "                \"score\": \"\" if part_name != \"Project Overview\" else \" \",\n",
    "                \"explanation\": \"\" if part_name != \"Project Overview\" else \" \"\n",
    "            }},\n",
    "            \"Timeline\": {{\n",
    "                \"score\": \"\" if part_name != \"Timeline\" else \" \",\n",
    "                \"explanation\": \"\" if part_name != \"Timeline\" else \" \"\n",
    "            }},\n",
    "            \"Project Scope\": {{\n",
    "                \"score\": \"\" if part_name != \"Project Scope\" else \" \",\n",
    "                \"explanation\": \"\" if part_name != \"Project Scope\" else \" \"\n",
    "            }},\n",
    "            \"Project Team\": {{\n",
    "                \"score\": \"\" if part_name != \"Project Team\" else \" \",\n",
    "                \"explanation\": \"\" if part_name != \"Project Team\" else \" \"\n",
    "            }},\n",
    "            \"Total Score\": \"\",\n",
    "            \"Overall Description\": \"\"\n",
    "        }}\n",
    "    4. When you return the result, please include the total score and overall description.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Split the text by predefined parts (titles)\n",
    "def split_text_by_parts(text: str, output_dir: str):\n",
    "    part_titles = [\n",
    "        \"Project Description / Purpose\",\n",
    "        \"Project Overview\",\n",
    "        \"Timeline\",\n",
    "        \"Project Scope\",\n",
    "        \"Project Team\"\n",
    "    ]\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)  # Create the directory if it does not exist\n",
    "\n",
    "    chunks = {}\n",
    "    current_part = None\n",
    "    buffer = []\n",
    "    title_processed = set()\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        # Detect part titles (only if it's a standalone title line)\n",
    "        if stripped_line in part_titles and line == stripped_line and stripped_line not in title_processed:\n",
    "            if current_part:\n",
    "                # Replace '/' with '_' in the current part name for the filename\n",
    "                safe_part_name = current_part.replace(\"/\", \"_\")\n",
    "                chunks[current_part] = \"\\n\".join(buffer)  # Save the content of the previous part\n",
    "                # Save the chunk to a txt file(Debug)\n",
    "                with open(os.path.join(output_dir, f\"{safe_part_name}.txt\"), \"w\") as f:\n",
    "                    f.write(chunks[current_part])\n",
    "\n",
    "            current_part = stripped_line  # Start a new part\n",
    "            buffer = []  # Reset the buffer for the new part\n",
    "            title_processed.add(current_part)\n",
    "        elif current_part:  # Continue accumulating lines for the current part\n",
    "            buffer.append(line)\n",
    "\n",
    "    if current_part:  # Save the last part\n",
    "        # Replace '/' with '_' in the current part name for the filename\n",
    "        safe_part_name = current_part.replace(\"/\", \"_\")\n",
    "        chunks[current_part] = \"\\n\".join(buffer)\n",
    "        # Save the last chunk to a txt file (Debug)\n",
    "        with open(os.path.join(output_dir, f\"{safe_part_name}.txt\"), \"w\") as f:\n",
    "            f.write(chunks[current_part])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Evaluate each part\n",
    "def evaluate_part(text: str, rubric: dict, part_name: str, chain):\n",
    "    prompt = generate_prompt(rubric, part_name)  # Make prompt for each part\n",
    "    document = Document(page_content=text)\n",
    "\n",
    "    try:\n",
    "        with get_openai_callback() as callback:\n",
    "            answer = chain.run(input_documents=[document], question=prompt)\n",
    "            print(f\"LLM evaluation result for {part_name}: {answer}\")\n",
    "\n",
    "            # Extract JSON part from LLM response\n",
    "            json_start_index = answer.find(\"{\")\n",
    "            json_end_index = answer.rfind(\"}\") + 1\n",
    "            if json_start_index == -1 or json_end_index == -1:\n",
    "                print(f\"No valid JSON found in LLM response for {part_name}: {answer}\")\n",
    "                return None\n",
    "\n",
    "            json_data = answer[json_start_index:json_end_index]\n",
    "\n",
    "            # Parsing JSON data\n",
    "            try:\n",
    "                result = yaml.safe_load(json_data)\n",
    "                print(f\"Parsed JSON for {part_name}: \\n {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing JSON for {part_name}: {e}\")\n",
    "                print(f\"Invalid JSON content: {json_data}\")\n",
    "                return None\n",
    "\n",
    "            # Extract total score and overall description for each part\n",
    "            total_score = result.get(\"Total Score\", \"\")\n",
    "            overall_description = result.get(\"Overall Description\", \"\")\n",
    "\n",
    "            return {\n",
    "                \"score\": total_score,\n",
    "                \"explanation\": overall_description\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running LLM for {part_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Dictionary for saving the evaluation results of each part\n",
    "evaluation_results = {\n",
    "    \"Project Description / Purpose\": {\"score\": \"\", \"explanation\": \"\"},\n",
    "    \"Project Overview\": {\"score\": \"\", \"explanation\": \"\"},\n",
    "    \"Timeline\": {\"score\": \"\", \"explanation\": \"\"},\n",
    "    \"Project Scope\": {\"score\": \"\", \"explanation\": \"\"},\n",
    "    \"Project Team\": {\"score\": \"\", \"explanation\": \"\"},\n",
    "    \"Total Score\": \"\",\n",
    "    \"Overall Description\": \"\"\n",
    "}\n",
    "\n",
    "\n",
    "# Aggregating the total evaluation\n",
    "def aggregate_results():\n",
    "    total_score = 0\n",
    "    for part in evaluation_results:\n",
    "        if evaluation_results[part][\"score\"]:\n",
    "            try:\n",
    "                total_score += int(evaluation_results[part][\"score\"])  # Sum up scores\n",
    "            except ValueError:\n",
    "                pass\n",
    "    evaluation_results[\"Total Score\"] = f\"{total_score}/100\"\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "\n",
    "# Evaluate the document based on the predefined rubric\n",
    "def evaluate_document(text: str):\n",
    "    print(\"evaluate_document function is called\")\n",
    "\n",
    "    # Load rubric from YAML file\n",
    "    rubric_file = os.path.join(\"Prompts\", \"rubric.yaml\")\n",
    "    rubric = load_rubric(rubric_file)\n",
    "\n",
    "    # Split the text into parts based on the titles\n",
    "    parts = split_text_by_parts(text, output_dir)\n",
    "\n",
    "    try:\n",
    "        llm = ChatOllama(model=\"llama3\", temperature=0)\n",
    "        print(\"LLM is initialized and running\")\n",
    "        chain = load_qa_chain(llm=llm)\n",
    "\n",
    "        # Process each part individually using the generated prompt\n",
    "        with get_openai_callback() as callback:\n",
    "            for part_name, part_text in parts.items():\n",
    "                print(f\"Evaluating part: {part_name}\")\n",
    "                part_result = evaluate_part(part_text, rubric, part_name, chain)\n",
    "                if part_result:\n",
    "                    evaluation_results[part_name] = part_result\n",
    "\n",
    "        final_result = aggregate_results()\n",
    "\n",
    "        return final_result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception in LLM evaluation: \" + str(e))\n",
    "\n",
    "    return \"Unexpected error\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
